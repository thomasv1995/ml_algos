{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3b2eef-da55-46dd-8d25-66663be0da6b",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3fc5fe-6556-4b2d-9c91-3ffc1afdbc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7892e8fc-166b-4ed7-bb5a-09230274ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = \"My name is Ajay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ad5ccd-5d79-4ccf-add6-b5b8cdc0c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4 # length of the input sequence in terms of words\n",
    "d_k = 8 # number of dimensions in the key vector\n",
    "d_v = 8 #  number of dimensions in the value vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f090a9-56a9-4e72-a6e0-519ce2fb3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate for each word a query, key, and value vector\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358a879-45e9-430d-8902-69b558a6b38a",
   "metadata": {},
   "source": [
    " For word \"My\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d6f4a37-7ff8-45d4-8643-7f6407dca83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word : 'My'\n",
      "Q\n",
      " [ 1.60870184 -0.33850311  0.61360379  0.90066393 -1.17032812 -0.08006709\n",
      " -0.74460439 -0.84119747]\n",
      "K\n",
      " [ 0.01330805 -0.80453143  0.13958882 -1.83548747 -2.00376175 -0.6174246\n",
      "  0.5314595  -0.44841173]\n",
      "V\n",
      " [ 0.01330805 -0.80453143  0.13958882 -1.83548747 -2.00376175 -0.6174246\n",
      "  0.5314595  -0.44841173]\n",
      "----------------------------\n",
      "\n",
      "word : 'name'\n",
      "Q\n",
      " [ 1.60870184 -0.33850311  0.61360379  0.90066393 -1.17032812 -0.08006709\n",
      " -0.74460439 -0.84119747]\n",
      "K\n",
      " [ 0.01330805 -0.80453143  0.13958882 -1.83548747 -2.00376175 -0.6174246\n",
      "  0.5314595  -0.44841173]\n",
      "V\n",
      " [ 0.01330805 -0.80453143  0.13958882 -1.83548747 -2.00376175 -0.6174246\n",
      "  0.5314595  -0.44841173]\n",
      "----------------------------\n",
      "\n",
      "word : 'is'\n",
      "Q\n",
      " [ 1.60870184 -0.33850311  0.61360379  0.90066393 -1.17032812 -0.08006709\n",
      " -0.74460439 -0.84119747]\n",
      "K\n",
      " [ 0.01330805 -0.80453143  0.13958882 -1.83548747 -2.00376175 -0.6174246\n",
      "  0.5314595  -0.44841173]\n",
      "V\n",
      " [ 0.01330805 -0.80453143  0.13958882 -1.83548747 -2.00376175 -0.6174246\n",
      "  0.5314595  -0.44841173]\n",
      "----------------------------\n",
      "\n",
      "word : 'Ajay'\n",
      "Q\n",
      " [ 1.60870184 -0.33850311  0.61360379  0.90066393 -1.17032812 -0.08006709\n",
      " -0.74460439 -0.84119747]\n",
      "K\n",
      " [ 0.01330805 -0.80453143  0.13958882 -1.83548747 -2.00376175 -0.6174246\n",
      "  0.5314595  -0.44841173]\n",
      "V\n",
      " [ 0.01330805 -0.80453143  0.13958882 -1.83548747 -2.00376175 -0.6174246\n",
      "  0.5314595  -0.44841173]\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in input_seq.split(\" \"):\n",
    "    print(f\"word : '{word}'\")\n",
    "    print(\"Q\\n\", q[0])\n",
    "    print(\"K\\n\", k[0])\n",
    "    print(\"V\\n\", k[0])\n",
    "    print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f3fe7-c17d-4d25-ab5a-fbb98829dfc8",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "Create an attention matrix to let every word within the input sequence look at every single other word to see if it has a higher affinity towards it or not\n",
    "\n",
    "$$softmax(\\frac{Q.K^T}{\\sqrt(d_k)} + M)V $$\n",
    "\n",
    "where : \n",
    "- $Q$ = what I am looking for\n",
    "- $K$ = what i currently have\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80206ce-a74e-4943-8803-f69bdd0e0ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.10220978,  1.44093158,  0.9190615 ,  0.26651833],\n",
       "       [-1.83120489, -4.00947714,  1.85162327,  3.19053453],\n",
       "       [ 0.96064523, -0.32591597,  1.32951125, -1.43763971],\n",
       "       [-0.6289311 ,  2.33419412, -1.39226873, -0.39662222]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4e386-d0f9-4868-896a-07b7fe77209b",
   "metadata": {},
   "source": [
    "The product $Q.K^T$ leads to a 4x4 matrix displaying values proportional to how much attention we want to focus on each word. The values of the first ROW of the matrix show how much attention we pay to each word when the query = 'My' (the largest being for the word itself, \"My\"), second row is when query = \"name\", etc.\n",
    "\n",
    "$\\sqrt(d_k)$ is used to minimize the variance of $Q.K^T$, as per the paper Attention is All you Need \"the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients\". Helps to stabilize the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea998b-635f-4478-ba6d-c52c5bcbdd20",
   "metadata": {},
   "source": [
    "**before scaling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa2b3bc-51f6-446e-aedd-12ece3b77c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.083509890285003)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc0ee1-9fd9-4c73-93bf-096d0bc7badd",
   "metadata": {},
   "source": [
    "**after scaling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "972ee7a3-1bec-4a62-8a49-f40bb8b22243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.38543873628562525)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_matrix_scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "attention_matrix_scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29cfef-352a-4c7c-939b-851d6adf24c1",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Masking words ahead of the current input word is required for the decoder (as, in the real world, we cannot predict a word based on the predictions that lie ahead of it. However, this restriction is not necessary when modelling embeddings using the encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "654aacc1-9572-4755-8032-4d0a44bf151d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09332aff-67c1-4c60-a83d-6e8d93c5c691",
   "metadata": {},
   "source": [
    "The above matrix simulates the fact that a given word can only see the words that have come before it. \"My\" can only see \"My\", \"name\" can only see \"My\" and \"name\", etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc5d4f49-dfd2-43cf-82c8-d818d5c2d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask == 0] = -np.inf \n",
    "mask[mask==1] = 0 # set to 0 as we will be replacing them with the self attention values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f097f86-f000-4470-a30c-0aabc083f243",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721bfcc-9b8d-4baa-91e3-a9bafcaf49ea",
   "metadata": {},
   "source": [
    "$$\\frac{e^{x_i}}{\\sum_{j}e^x_j}$$\n",
    "\n",
    "For a given word, get the probability distribution of all its attention values. To do so, divide each exponentiated attention value by the sum of all exponentiated attention value. Repeat for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63cff684-62a1-46e7-a9a5-5c5f49d0ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21491d2f-218e-4074-82ee-91a768094450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.65257547, 0.34742453, 0.        , 0.        ],\n",
       "       [0.72320796, 0.05172176, 0.22507028, 0.        ],\n",
       "       [0.08825281, 0.36919315, 0.28033537, 0.26221867]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(attention_matrix_scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dda3a38d-b3c0-4fa7-bf67-ce6240b75396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.93507226, 0.58845159, 4.99045031, 0.34683577],\n",
       "       [0.        , 0.31328563, 0.3569027 , 1.45093838],\n",
       "       [0.        , 0.        , 1.5530831 , 1.10172509],\n",
       "       [0.        , 0.        , 0.        , 1.03052599]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(x).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd1b3e8e-4c31-4b30-92fd-5d7d457d2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = attention_matrix_scaled + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b0b9323-daef-4fc6-bcef-c5ed117b9afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.93507226, 0.90173721, 6.90043611, 3.93002524])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.exp(x), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa42d1-bd34-444f-8b40-49c928c03194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_algos_env",
   "language": "python",
   "name": ".ml_algos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
