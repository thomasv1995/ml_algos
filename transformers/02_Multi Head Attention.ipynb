{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82c12ec-b3ef-4b5f-9aaa-ea148b7cf20e",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=HQn1QKQYXVg&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4&index=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fab4ff-ee47-4584-9bde-11782ae2d208",
   "metadata": {},
   "source": [
    "![pic](../doc/img//multi_head_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df34a56-ea79-4dec-abd7-3bd8aa2bca7a",
   "metadata": {},
   "source": [
    "each word in the input sequence will have three types of vectors associated to it :\n",
    "- $q$ = query vector, represents what I am looking for\n",
    "- $k$ = what I can offer\n",
    "- $v$ = what I actually offer\n",
    "\n",
    "each vector has the same number of dimensions as the embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae57e9-5649-4a79-a05a-3a2597daae53",
   "metadata": {},
   "source": [
    "The q, k, v vectors are then split into multiple attention heads of equal size. In the paper, they use 8 attention heads, so each vector is split into 8 vectors of size 1x64 (given an embedding dimension of 512).\n",
    "\n",
    "Then for each head, we generate an attention matrix based on the heads of other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42e3a31-2511-4b6a-af67-f8d807f79b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f62f0c-dc0a-4a5a-a377-e9dd71504c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4 # typically you would set a mximum sequence length so that all vectors are of the same size\n",
    "batch_size = 1 \n",
    "input_dim = 512\n",
    "d_model = 512 # output of the attention unit for every single word\n",
    "# randomly sampled input, represents the value that is inputed into the multi head attention block in the transforme architecture\n",
    "x = torch.randn((batch_size, sequence_length, input_dim)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098dcd58-1d9c-4622-81a7-1e95335b0530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2875,  0.4003, -1.5862,  ...,  0.4906,  0.1627,  0.7256],\n",
       "         [-0.2272, -0.6080,  1.0705,  ..., -0.1585, -1.4422,  0.3697],\n",
       "         [ 0.9237,  0.2495,  1.9846,  ...,  1.0509,  0.0411,  1.3766],\n",
       "         [ 0.0165, -0.1440,  1.4073,  ...,  0.0226, -0.4571,  1.0191]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0408475f-6f4d-46f7-9183-3ee2318bd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated q,k, v vectors for all attention heads\n",
    "qkv_layer = nn.Linear(input_dim, 3 * d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80741108-53c9-45cd-b2c0-355c3c4a81e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1536, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f693bb6-7342-4b16-bf45-cb2f76e5d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = qkv_layer(x) # pass input to the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3d7df2-25ca-4818-bbfd-dddb769db80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7fa77e5-e7f1-45a1-b14b-658c59baffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads # 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49534e0-ec54-4b36-89fa-27b42e0d0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split qkv vectors by attention head by adding a 4 th dimension\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8247fdb-1d2f-4989-abf5-e1674b3fb460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59ecee31-26f7-4852-bdde-20dbebfdd7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3) # put the head dimension before the sequence length dimension for parallelization on the last 2 dimensiosn\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "770aeca3-b16a-4b4d-99c6-9e74840c7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split separate q, k, v vectors\n",
    "q, k, v = qkv.chunk(3, dim=-1) # -1 = last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf73c310-fb72-490a-98b4-c058dba938a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6714c-93c9-47cf-b315-8249567754b7",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "self attention = $softmax(\\frac{Q.K^T}{\\sqrt(d_k)} + M) $  \n",
    "new V = self attention.V\n",
    "\n",
    "Every word has a query vector, and it will compare its query (Q) vector to every other word's key (K) vector to get its attention values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfc6b2c1-f0a2-4b42-a134-560c7dcb0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = q.size()[-1] # 64\n",
    "# must use k.transpose instead of k.T, since it is 4 dimensional and we want to specify only a subset of dimension on which to transpose\n",
    "# here we transpose on the last 2 dimensions which are the sequence length and the head dimension size \n",
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # attention matrices for all heads within each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535cd274-be82-4005-a598-bf9e271547dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93525075-6b58-40f4-a7a1-fd2623ba18d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(scaled.size(), float('-inf'))\n",
    "mask = torch.triu(mask, \n",
    "                  diagonal =1) # sepcifies how many diagonals above the 0's to replace with 0's\n",
    "mask[0][0] # mask for a single head\n",
    "# we use -inf so that the exponents of -inf become 0's in the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08259c13-d961-4fad-bf45-9eaa75837b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2036,    -inf,    -inf,    -inf],\n",
       "        [ 0.7623,  0.3230,    -inf,    -inf],\n",
       "        [-0.3756,  0.3354, -0.0095,    -inf],\n",
       "        [-0.3710, -0.1733, -0.2924,  0.0059]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled + mask)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb3cd2c2-156b-4141-ab1a-5341f1335ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the attention values for the decoder so that each word cannot only fetch information from words appearing before it\n",
    "# ensure the autoregressive nature of a translation task\n",
    "scaled += mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b206b403-52a0-4ea1-83fe-9ab6096a6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = F.softmax(scaled, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e5b7843-a9c6-4a76-b410-463fcd6fd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated the new value vector that are more context aware then its previous values\n",
    "values = torch.matmul(attention, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08c7c87d-3d44-4827-acca-c383c8a2aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f59986c-5a05-4baa-83d5-77fd2ee1a40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc57ec2b-85a5-47a8-af3d-999174392335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconcatenate value vectors along the number of heads dimension\n",
    "values = values.reshape(batch_size, sequence_length, num_heads * head_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "100ad912-38ef-4287-aff3-874389743eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd70a03-97ce-4e54-b283-f368dbad18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable heads to communicate with each other by passing them through a feedforward layer\n",
    "linear_layer = nn.Linear(d_model, d_model)\n",
    "out = linear_layer(values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66883eb5-ea2b-4fc1-8977-2edb1f909e95",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff77b570-8aab-47a0-8cad-43817d8c3bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all self attention lines of code above into one function\n",
    "# note that mask is optional to reflect the fact self attention can be calculated on either the encoder (no mask) or the decoder (mask)\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1] # 64\n",
    "    # must use k.transpose instead of k.T, since it is 4 dimensional and we want to specify only a subset of dimension on which to transpose\n",
    "    # here we transpose on the last 2 dimensions which are the sequence length and the head dimension size \n",
    "    scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # attention matrices for all heads within each batch\n",
    "    if mask:\n",
    "        # mask the attention values for the decoder so that we each word cannot only fetch information from words appearing before it\n",
    "        # ensure the autoregressive nature of a translation task\n",
    "        scaled += mask\n",
    "    \n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    # generated the new value vector that are more context aware then its previous values\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e976948c-1043-4610-b1be-7448d6913796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        # get concatenated qkv vectors, this time split by attention head\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 *self. head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # put the head dimension before the sequence length dimension for parallelization on the last 2 dimensiosn\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        # now split separate q, k, v vectors\n",
    "        q, k, v = qkv.chunk(3, dim=-1) # -1 = last dimension\n",
    "        print(f\"q.size(): {q.size()} k.size(): {k.size()}, v.size(): {v.size()}\")\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        print(f\"values.size(): {values.size()}, attention.size(): {attention.size()}\")\n",
    "        # reconcatenate value vectors along the number of heads dimension\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size() : {out.size()}\")\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be5b73e9-977e-45de-a165-9019108cb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1024\n",
    "input_dim = 512\n",
    "num_head = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "\n",
    "x = torch.randn((batch_size, sequence_length, input_dim)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6865369-f629-4db3-a174-4dce18005d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 512])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q.size(): torch.Size([30, 8, 5, 64]) k.size(): torch.Size([30, 8, 5, 64]), v.size(): torch.Size([30, 8, 5, 64])\n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention.size(): torch.Size([30, 8, 5, 5])\n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size() : torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "model = MultiheadAttention(input_dim, d_model, num_head) \n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58872abd-d230-4b82-8812-fbafba6e33ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.2482e-03,  1.8415e-01,  1.3552e-01,  ..., -3.3307e-01,\n",
       "           8.7331e-02,  1.9772e-01],\n",
       "         [-2.4796e-01,  1.9846e-01,  2.6852e-02,  ...,  1.5789e-02,\n",
       "           9.6477e-03,  3.0723e-02],\n",
       "         [ 6.4421e-02, -1.4961e-01, -1.7929e-02,  ...,  2.6415e-01,\n",
       "          -8.2693e-02,  7.8689e-02],\n",
       "         [ 7.5609e-02, -2.8181e-01, -1.5872e-01,  ...,  1.2011e-02,\n",
       "          -1.2643e-03, -5.6821e-02],\n",
       "         [ 1.8020e-01,  3.6276e-01, -2.0193e-01,  ...,  2.4924e-01,\n",
       "          -8.4760e-02, -4.8158e-01]],\n",
       "\n",
       "        [[ 2.3121e-02,  5.3049e-02,  1.3574e-01,  ..., -1.1809e-03,\n",
       "           1.6767e-01, -1.9779e-01],\n",
       "         [ 2.0019e-01,  1.6813e-01, -7.7353e-02,  ...,  2.4081e-01,\n",
       "           3.4660e-02,  4.4718e-02],\n",
       "         [-1.5035e-01,  1.6051e-01,  2.4635e-01,  ...,  5.8178e-02,\n",
       "           2.5456e-01,  7.6498e-02],\n",
       "         [ 2.8846e-01, -4.7870e-02,  1.2407e-01,  ...,  1.4104e-01,\n",
       "          -1.8601e-01, -3.1178e-01],\n",
       "         [-8.7335e-02, -2.8972e-01, -7.5568e-03,  ..., -1.3116e-01,\n",
       "           8.2856e-02, -1.1968e-01]],\n",
       "\n",
       "        [[ 1.8129e-01,  2.8328e-01, -1.7378e-01,  ...,  2.7540e-02,\n",
       "          -1.6753e-01, -2.1008e-01],\n",
       "         [-1.3002e-01,  1.0170e-01,  2.0209e-02,  ...,  2.6526e-01,\n",
       "           7.2850e-03, -2.2354e-01],\n",
       "         [ 1.2200e-01,  5.2713e-02, -1.2907e-01,  ...,  5.5027e-02,\n",
       "           1.4915e-01,  2.4490e-01],\n",
       "         [-5.2647e-02,  2.5397e-05,  3.2546e-02,  ..., -1.0367e-01,\n",
       "           1.0772e-01,  2.5024e-01],\n",
       "         [-9.2997e-02,  9.0789e-02, -9.7633e-03,  ...,  9.4699e-02,\n",
       "           4.6475e-02,  9.4455e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.2835e-01, -4.3956e-02, -1.9255e-01,  ..., -5.7907e-02,\n",
       "          -3.3029e-01,  6.6557e-02],\n",
       "         [ 1.4038e-01,  2.1148e-01, -5.6308e-02,  ..., -8.4683e-02,\n",
       "           1.1365e-01,  1.3563e-01],\n",
       "         [ 1.1008e-01,  1.0161e-01,  2.0771e-01,  ...,  6.0491e-02,\n",
       "           3.2299e-01, -6.2508e-02],\n",
       "         [-2.1942e-02, -8.1924e-02, -3.1060e-01,  ...,  1.8708e-01,\n",
       "          -4.1513e-02, -3.2814e-01],\n",
       "         [ 9.6665e-02, -3.2902e-02, -1.0427e-01,  ..., -6.6730e-02,\n",
       "          -2.2419e-02,  3.1302e-01]],\n",
       "\n",
       "        [[-1.0821e-01,  4.9773e-01, -2.1170e-01,  ...,  1.7156e-01,\n",
       "          -4.8791e-04,  5.8492e-02],\n",
       "         [-1.3832e-01, -8.9967e-02,  2.6679e-02,  ...,  1.2294e-01,\n",
       "           2.0558e-01,  1.3180e-01],\n",
       "         [-1.9017e-01,  1.0176e-01, -1.7417e-01,  ..., -6.4282e-02,\n",
       "           1.0571e-02,  3.4158e-01],\n",
       "         [ 4.3138e-02,  3.6329e-02, -7.7917e-02,  ...,  7.9920e-02,\n",
       "          -9.5929e-02,  9.3966e-02],\n",
       "         [-1.3758e-01,  3.0321e-01, -1.0428e-01,  ...,  1.7396e-01,\n",
       "          -2.0032e-01,  2.7183e-03]],\n",
       "\n",
       "        [[-1.5948e-01,  1.1645e-01,  1.3377e-01,  ...,  2.2962e-02,\n",
       "           1.7760e-01,  1.2567e-01],\n",
       "         [ 5.4521e-02,  1.5003e-01, -8.1216e-02,  ..., -3.5223e-02,\n",
       "          -8.9680e-03, -5.4326e-02],\n",
       "         [ 1.0255e-01, -1.8632e-01,  1.6563e-01,  ..., -1.3659e-01,\n",
       "           5.9860e-02, -5.2372e-03],\n",
       "         [ 7.9697e-02, -1.4945e-02,  8.7959e-02,  ...,  1.5293e-01,\n",
       "           1.9332e-01, -1.4150e-01],\n",
       "         [-2.1858e-01,  3.7265e-01, -1.0055e-01,  ...,  3.3056e-01,\n",
       "          -6.5990e-02,  3.2630e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b8007c-122b-4129-8b54-eb462cd595b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_algos_env",
   "language": "python",
   "name": ".ml_algos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
